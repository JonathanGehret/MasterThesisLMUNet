{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMUNet: Landscape metrics unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ORIGINAL SOURCE:** [Top 100 solution - SIIM-ACR Pneumothorax Segmentation](https://amaarora.github.io/2020/09/06/siimacr.html#train-and-valid-augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data-structure should look like this (old names: \n",
    "\n",
    "```\n",
    "├── data\n",
    "│   ├── dataset512\n",
    "│   ├── dicom-images-test\n",
    "│   └── dicom-images-train\n",
    "└── src\n",
    "    └── pneumothorax-segmentation \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)  Imports\n",
    "\n",
    "remove obsolete ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "#import segmentation_models_pytorch as smp\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "#from torch.nn import MSELoss\n",
    "from collections import defaultdict\n",
    "import torchvision\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from torch.utils.data.sampler import Sampler\n",
    "import sys; sys.path.append('../pneumothorax-segmentation/unet_pipeline/')\n",
    "#from Losses import ComboLoss, dice_metric\n",
    "#os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "try:\n",
    "    get_ipython().__class_._name__\n",
    "    from tqdm.notebook import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Config (Hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [ComboLoss](https://github.com/sneddy/pneumothorax-segmentation/blob/master/unet_pipeline/Losses.py#L104) function used in CRITERION below also comes from the winning solution by Anuar. \n",
    "\n",
    "```\n",
    "Adjust:\n",
    "    - file paths\n",
    "    - batch size\n",
    "    - learning rate\n",
    "    - epochs\n",
    "    - criterion (=loss function)\n",
    "    - train or evaluate\n",
    "    - etc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE         = 128\n",
    "#DATA_DIR         = Path(f'../data/dataset{IMG_SIZE}/')\n",
    "DATA_DIR         = Path(f'../notebooks/data')\n",
    "#DATA_DIR         = Path(f'/data')\n",
    "TRAIN_IMG_DIR    = DATA_DIR/'100k_landscapes_no_classes'#for 100k sidi 3 class\n",
    "#TRAIN_IMG_DIR    = DATA_DIR/'100k_landscapes_5_class'\n",
    "#TRAIN_LBL_DIR    = DATA_DIR/'npy_10000_random_moving_window_sidi'\n",
    "#TRAIN_LBL_DIR    = DATA_DIR/'npy_3_class_lsm_l_sidi_5_mov_win'\n",
    "TRAIN_LBL_DIR    = DATA_DIR/'100k_sidi_3_class_2empty_channels'\n",
    "#TRAIN_LBL_DIR    = DATA_DIR/'100k_joinent_frac_mn_5_class_stacked'\n",
    "#RLE_DF           = pd.read_csv('../data/train-rle.csv', names=['ImageId', 'EncodedPixels'], skiprows=1)\n",
    "#RLE_DF           = pd.read_csv('data/metric_list_reti_futu.csv', names=['ImageId', 'EncodedPixels'], skiprows=1)\n",
    "#RLE_DF           = pd.read_csv('data/100k_ls_metr.csv',  names=['ImageId', 'EncodedPixels'], skiprows=1)\n",
    "#RLE_DF           = pd.read_csv('data/100k_ls_metr.csv', header=None)\n",
    "#RLE_DF           = pd.read_csv('data/metric_list_reti_futu.csv', header=None)\n",
    "#TRAIN_DF           = pd.read_csv('data/100k_frac_mn_5_class_metric_list.csv', header=None)\n",
    "TRAIN_DF           = pd.read_csv('data/100k_ls_metr.csv', header=None)\n",
    "#KFOLD_PATH       = DATA_DIR/'RLE_kfold.csv'\n",
    "TRAIN_BATCH_SIZE = 8 #[8, 14, 16, 32]\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "USE_SAMPLER      = False\n",
    "#POSTIVE_PERC     = 0.8\n",
    "#ENCODER          = 'se_resnext50_32x4d'\n",
    "#ENCODER_WEIGHTS  = 'imagenet'\n",
    "#CLASSES          = ['mask']\n",
    "#METRIC_NAMES     = ['lsm_l_sidi','lsm_l_joinent']\n",
    "#NUM_METRICS      = len(['lsm_l_sidi','lsm_l_joinent'])\n",
    "#ACTIVATION       = None \n",
    "DEVICE           = 'cuda'\n",
    "#PRETRAINED_PATH  = '../data/bst_model512_fold2_0.9565.bin'\n",
    "#PRETRAINED_PATH  = 'data/saved_models/first_working_evaluation_score/bst_model128_fold4_9.999999747378752e-05.bin'\n",
    "PRETRAINED_PATH  = 'outputs/saved_models/100k_frac_mn_joinent_stacked_5_class/bst_model128_fold8_0.8402000069618225.bin'\n",
    "PRETRAINED       = True\n",
    "LEARNING_RATE    = 2e-5\n",
    "#LEARNING_RATE    = 0.1\n",
    "#LEARNING_RATE    = 0.01\n",
    "#EPOCHS           = 50\n",
    "EPOCHS           = 100\n",
    "#LOSS_FN          = 'mixed'\n",
    "#LOSS_FN          = 'MSE'\n",
    "#LOSS_FN          = 'RMSE'\n",
    "LOSS_FN          = 'MAE'\n",
    "#LOSS_FN          = 'R2'\n",
    "#LOSS_FN           = 'pixels'\n",
    "#EVAL_FN         = 'MSE'\n",
    "EVAL_FN         = 'R2'\n",
    "#CRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\n",
    "USE_CRIT         = True\n",
    "TRAIN_MODEL      = True\n",
    "TEST_MODEL       = False\n",
    "#EVALUATE         = False\n",
    "#KFOLD_SPLITS     = 10\n",
    "#FOLD_IDS          = range(KFOLD_SPLITS-1)\n",
    "FOLD_ID          = 8\n",
    "#TEST_ID           = 9\n",
    "#NUM_WORKERS      = 4\n",
    "NUM_WORKERS      = 12\n",
    "GRID_SEARCH      = False\n",
    "GRID_SEARCH_OPTIM      = False\n",
    "NUM_METRICS = 2 # 3rd dimension added.\n",
    "NUM_METRICS_NEW = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    fig,ax = plt.subplots(figsize=(8,8))\n",
    "    ax.imshow(img.permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(landscape, metric, left_title = \"Landscape\", right_title = \"Metric\"):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    #landscapes = {k:v.numpy() for k,v in landscapes.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy \n",
    "    #landscape = landscape.numpy()\n",
    "    #metric = metric.numpy()\n",
    "    #n = len(landscapes)\n",
    "    columns = 2\n",
    "    rows = 1\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.title(left_title)\n",
    "    #plt.title(\"Landscape\")\n",
    "    #landscape, metric = landscapes['landscape'], landscapes['metric']\n",
    "    #plt.imshow(landscape.transpose(1,2,0), vmin=0, vmax=1)\n",
    "    plt.imshow(landscape)\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.title(right_title)\n",
    "    #plt.title(\"Metric\")\n",
    "    plt.imshow(metric)\n",
    "    #if metric.max()>0:\n",
    "    #    plt.imshow(metric.squeeze(0), alpha=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Create k-fold splits - empty right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# create k folds train file if it doesn't exist already\\nif not os.path.exists(KFOLD_PATH):\\n    RLE_DF['has_mask'] = 0\\n    RLE_DF.loc[RLE_DF.EncodedPixels!='-1', 'has_mask'] = 1\\n    kf = StratifiedKFold(n_splits=KFOLD_SPLITS)\\n    RLE_DF['kfold']=-1\\n    for fold, (train_index, test_index) in enumerate(kf.split(X=RLE_DF.ImageId, y=RLE_DF.has_mask)):\\n            RLE_DF.loc[test_index, 'kfold'] = fold\\n    RLE_DF.to_csv(KFOLD_PATH, index=False)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# create k folds train file if it doesn't exist already\n",
    "if not os.path.exists(KFOLD_PATH):\n",
    "    RLE_DF['has_mask'] = 0\n",
    "    RLE_DF.loc[RLE_DF.EncodedPixels!='-1', 'has_mask'] = 1\n",
    "    kf = StratifiedKFold(n_splits=KFOLD_SPLITS)\n",
    "    RLE_DF['kfold']=-1\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X=RLE_DF.ImageId, y=RLE_DF.has_mask)):\n",
    "            RLE_DF.loc[test_index, 'kfold'] = fold\n",
    "    RLE_DF.to_csv(KFOLD_PATH, index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#for FOLD_ID in FOLD_IDS:\\nDF       = pd.read_csv(KFOLD_PATH)\\nTRAIN_DF = DF.query(f'kfold!={FOLD_ID} and kfold!={TEST_ID}').reset_index(drop=True)\\nVAL_DF   = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\\nTEST_DF  = DF.query(f'kfold=={TEST_ID}').reset_index(drop=True)\\n#print(len(TRAIN_DF), len(VAL_DF), len(TEST_DF))\\n#print(TRAIN_DF.ImageId[0], VAL_DF.ImageId[0], TEST_DF.ImageId[0])\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#for FOLD_ID in FOLD_IDS:\n",
    "DF       = pd.read_csv(KFOLD_PATH)\n",
    "TRAIN_DF = DF.query(f'kfold!={FOLD_ID} and kfold!={TEST_ID}').reset_index(drop=True)\n",
    "VAL_DF   = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)\n",
    "TEST_DF  = DF.query(f'kfold=={TEST_ID}').reset_index(drop=True)\n",
    "#print(len(TRAIN_DF), len(VAL_DF), len(TEST_DF))\n",
    "#print(TRAIN_DF.ImageId[0], VAL_DF.ImageId[0], TEST_DF.ImageId[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Data augmentation (\"transforms\") - not working right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Train transforms\\nTFMS = albu.Compose([\\n    ToTensorV2(),\\n    #albu.HorizontalFlip(always_app), # doesn't really matter?\\n    #albu.Rotate(limit=10),  # rotate could be fine, but needs to be the same\\n    #albu.Normalize(mean=[0.5], std=[0.5], max_pixel_value=1), # normalize also fine\\n    #ToTensorV2()\\n    ],\\n    #additional_targets={'metric': 'metric'}\\n)\\n\\n# Test transforms\\nVAL_TFMS = albu.Compose([\\n    #albu.Normalize(),\\n    ToTensorV2(),\\n])\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Train transforms\n",
    "TFMS = albu.Compose([\n",
    "    ToTensorV2(),\n",
    "    #albu.HorizontalFlip(always_app), # doesn't really matter?\n",
    "    #albu.Rotate(limit=10),  # rotate could be fine, but needs to be the same\n",
    "    #albu.Normalize(mean=[0.5], std=[0.5], max_pixel_value=1), # normalize also fine\n",
    "    #ToTensorV2()\n",
    "    ],\n",
    "    #additional_targets={'metric': 'metric'}\n",
    ")\n",
    "\n",
    "# Test transforms\n",
    "VAL_TFMS = albu.Compose([\n",
    "    #albu.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# chat gpt's suggestions:\\n\\nimport torch\\nimport torchvision.transforms as transforms\\n\\n# Define the transforms to be applied to the input images\\n# problem: not applied evenly on landscapes and metrics...\\nTFMS = transforms.Compose([\\n    transforms.ToTensor(),\\n    #transforms.RandomHorizontalFlip(),  # flips kind of pointless?\\n    #transforms.RandomVerticalFlip(),\\n    #transforms.functional.vflip(),    \\n    #transforms.RandomRotation(30),\\n    #transforms.functional.rotate(angle=30), # angle seems nice\\n    #transforms.RandomCrop(size=128),  # crop? yeah seems good.\\n    #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), #pointless\\n    #transforms.GaussianBlur(kernel_size=3), #pointless\\n    #transforms.Normalize(mean=[0.5], std=[0.5])\\n])\\n\\nVAL_TFMS = transforms.Compose([\\n    transforms.ToTensor(),\\n    #transforms.Normalize(mean=[0.5], std=[0.5])\\n])\\n\\nTEST_TFMS = transforms.Compose([\\n    transforms.ToTensor()#,\\n    #transforms.Normalize(mean=[0.5], std=[0.5])\\n])\\n\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# chat gpt's suggestions:\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transforms to be applied to the input images\n",
    "# problem: not applied evenly on landscapes and metrics...\n",
    "TFMS = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.RandomHorizontalFlip(),  # flips kind of pointless?\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    #transforms.functional.vflip(),    \n",
    "    #transforms.RandomRotation(30),\n",
    "    #transforms.functional.rotate(angle=30), # angle seems nice\n",
    "    #transforms.RandomCrop(size=128),  # crop? yeah seems good.\n",
    "    #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), #pointless\n",
    "    #transforms.GaussianBlur(kernel_size=3), #pointless\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "VAL_TFMS = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "TEST_TFMS = transforms.Compose([\n",
    "    transforms.ToTensor()#,\n",
    "    #transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to imports\n",
    "#from Dataset_test import LandscapeMetricsDataset\n",
    "#from Dataset_test_kfold import LandscapeMetricsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_dataset = LandscapeMetricsDataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR)#, TFMS)# , TFMS)#, augmentation=transforms_train) \n",
    "#val_dataset   = LandscapeMetricsDataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR)#, VAL_TFMS)#, VAL_TFMS) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if loading multiple datasets from multiple folders with the same indices:\n",
    "#train_dataset_01 = LandscapeMetricsDataset(TRAIN_DF, TRAIN_IMG_DIR_01, TRAIN_LBL_DIR_01)#, TFMS)# , TFMS)#, augmentation=transforms_train) \n",
    "#train_dataset_02 = LandscapeMetricsDataset(TRAIN_DF, TRAIN_IMG_DIR_02, TRAIN_LBL_DIR_02)#, TFMS)# , TFMS)#, augmentation=transforms_train) \n",
    "#train_dataset = torch.utils.data.ConcatDataset(train_dataset_01, train_dataset_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = LandscapeMetricsDataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TFMS) \\nval_dataset   = LandscapeMetricsDataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TEST_TFMS) \\n\\n#landscapes_dataset = LandscapeMetricsDataset(ls_list=TRAIN_DF, landscape_dir=TRAIN_IMG_DIR, metrics_dir=TRAIN_LBL_DIR)\\n#landscapes_dataset = LandscapeMetricsDataset(ls_list=TRAIN_DF, landscape_dir=TRAIN_IMG_DIR, metrics_dir=TRAIN_LBL_DIR)\\n#landscapes_dataset = LandscapeMetricsDataset(ls_list=RLE_DF, landscape_dir=TRAIN_IMG_DIR, metrics_dir=TRAIN_LBL_DIR, augmentation=transforms_train)\\n#landscapes_dataset = LandscapeMetricsDataset(ls_list=RLE_DF, landscape_dir=TRAIN_IMG_DIR, metrics_dir=TRAIN_LBL_DIR, augmentation=TFMS)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_dataset = LandscapeMetricsDataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TFMS) \n",
    "val_dataset   = LandscapeMetricsDataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR, TEST_TFMS) \n",
    "\n",
    "#landscapes_dataset = LandscapeMetricsDataset(ls_list=TRAIN_DF, landscape_dir=TRAIN_IMG_DIR, metrics_dir=TRAIN_LBL_DIR)\n",
    "#landscapes_dataset = LandscapeMetricsDataset(ls_list=TRAIN_DF, landscape_dir=TRAIN_IMG_DIR, metrics_dir=TRAIN_LBL_DIR)\n",
    "#landscapes_dataset = LandscapeMetricsDataset(ls_list=RLE_DF, landscape_dir=TRAIN_IMG_DIR, metrics_dir=TRAIN_LBL_DIR, augmentation=transforms_train)\n",
    "#landscapes_dataset = LandscapeMetricsDataset(ls_list=RLE_DF, landscape_dir=TRAIN_IMG_DIR, metrics_dir=TRAIN_LBL_DIR, augmentation=TFMS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
    "# split dataset into training and testing. \n",
    "# rando generator seed for reproducabitly included!\n",
    "#from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# split dataset (size: 95954)\n",
    "#train_dataset, val_dataset, test_dataset = random_split(dataset=landscapes_dataset, lengths=[1000,84000,10955], generator=torch.Generator().manual_seed(42))\n",
    "#train_dataset, val_dataset, test_dataset = random_split(dataset=landscapes_dataset, lengths=[75000,10000,10954], generator=torch.Generator().manual_seed(42))\n",
    "#train_dataset, val_dataset, test_dataset = random_split(dataset=landscapes_dataset, lengths=[750,125,125], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "#landscape, metric = train_dataset[136]['landscape'], train_dataset[136]['metric']\n",
    "#landscape = train_dataset[1]['landscape']\n",
    "\n",
    "#landscape.shape, metric.shape\n",
    "# (torch.Size([1, 128, 128]), torch.Size([1, 128, 128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one with mask \n",
    "#visualize(train_dataset[0]['landscape'].squeeze(), train_dataset[0]['metric'].squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### g) Sampler\n",
    "\n",
    "Do I want/need sampler? Using randomsampler, which is applied by setting shuffle=True in DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h) DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# dataloaders\\n# shuffle = True means random sampler, which is fine.\\ntrain_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, \\n                              shuffle=True if not USE_SAMPLER else False, \\n                              num_workers=NUM_WORKERS, \\n                              sampler=SAMPLER if USE_SAMPLER else None)\\nval_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# dataloaders\n",
    "# shuffle = True means random sampler, which is fine.\n",
    "train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, \n",
    "                              shuffle=True if not USE_SAMPLER else False, \n",
    "                              num_workers=NUM_WORKERS, \n",
    "                              sampler=SAMPLER if USE_SAMPLER else None)\n",
    "val_dataloader   = DataLoader(val_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# sanity check\\nlandscapes, metrics = next(iter(train_dataloader))['landscape'], next(iter(train_dataloader))['metric']\\nlandscapes.shape, metrics.shape\\n# should look like this: (torch.Size([14, 3, 512, 512]), torch.Size([14, 1, 512, 512]))\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# sanity check\n",
    "landscapes, metrics = next(iter(train_dataloader))['landscape'], next(iter(train_dataloader))['metric']\n",
    "landscapes.shape, metrics.shape\n",
    "# should look like this: (torch.Size([14, 3, 512, 512]), torch.Size([14, 1, 512, 512]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train images\n",
    "#ls_grid = torchvision.utils.make_grid(landscapes[:9], nrow=3, normalize=True)\n",
    "#matplotlib_imshow(ls_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metr_grid = torchvision.utils.make_grid(metrics[:9], nrow=3, normalize=True)\n",
    "#matplotlib_imshow(metr_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Multi-metric dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset_test_many_metrics import LandscapeMetricsDataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_DF = pd.read_csv('data/metric_list_5_class_sidi_10k.csv', header=None)\n",
    "#DATA_DIR         = Path(f'../notebooks/data')\n",
    "#TRAIN_LS_DIR = DATA_DIR/'100k_landscapes_no_classes'\n",
    "#TRAIN_LS_DIR = DATA_DIR/'10k_sidi_5_class_ls'\n",
    "#TRAIN_METR_DIR = DATA_DIR/'metric_stacks_experiment'\n",
    "#TRAIN_BATCH_SIZE = 8\n",
    "#NUM_WORKERS = 12\n",
    "#NUM_METRICS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dataset = LandscapeMetricsDataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR)#, metr_norm=True)#, TFMS)# , TFMS)#, augmentation=transforms_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(dataset=metrics_dataset, \n",
    "                                                        #lengths=[80000, 15000, 5000],\n",
    "                                                        lengths=[70000, 20000, 5954],                                                       \n",
    "                                                        generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]['metric'][1].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, \n",
    "                              shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = DataLoader(val_dataset, TRAIN_BATCH_SIZE, \n",
    "                              shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 128, 128]), torch.Size([8, 3, 128, 128]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "landscapes, metrics = next(iter(train_dataloader))['landscape'], next(iter(train_dataloader))['metric']\n",
    "landscapes.shape, metrics.shape\n",
    "# should look like this: (torch.Size([14, 3, 512, 512]), torch.Size([14, 1, 512, 512]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel wise mse loss\n",
    "if LOSS_FN == 'pixels':\n",
    "    def pixel_wise_mse_loss(input, target):\n",
    "        return torch.mean((input - target)**2)\n",
    "    criterion = pixel_wise_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses\n",
    "#LOSS_FN          = 'RMSE'\n",
    "\n",
    "if LOSS_FN == 'MAE':\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "# MSE\n",
    "elif LOSS_FN == 'MSE' or LOSS_FN == 'RMSE':\n",
    "    criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspred by chatgpt for adding new channels to pretrained model\n",
    "if PRETRAINED:    \n",
    "    def custom_mae_loss(y_pred, y_true):\n",
    "        # Extract the 4th channel from the target tensor\n",
    "        y_true_3 = y_true[:, 2:3, :, :]\n",
    "        # Compute the mean squared error between the predicted and target 4th channels\n",
    "        mae_loss = nn.L1Loss()(y_pred, y_true_3)\n",
    "        return mae_loss\n",
    "\n",
    "    criterion = custom_mae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### j) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [PyTorch Segmentation Models](https://github.com/qubvel/segmentation_models.pytorch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport segmentation_models_pytorch as smp\\n#import segmentation_models_pytorch as smp\\n\\nmodel = smp.Unet(\\n    encoder_name=ENCODER, \\n    encoder_weights=ENCODER_WEIGHTS, \\n    classes=3,     \\n    #classes=len(CLASSES), \\n    activation=ACTIVATION,\\n)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original\n",
    "'''\n",
    "import segmentation_models_pytorch as smp\n",
    "#import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=3,     \n",
    "    #classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_test torch.Size([1, 64, 128, 128])\n",
      "encoder_test torch.Size([1, 64, 128, 128])\n",
      "encoder_test torch.Size([1, 128, 64, 64])\n",
      "encoder_test torch.Size([1, 256, 32, 32])\n",
      "encoder_test torch.Size([1, 512, 16, 16])\n",
      "decoder_test torch.Size([1, 64, 64, 64])\n",
      "unet_output_dim_test torch.Size([1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# importen just_unet\n",
    "from unet_num_metr import unet, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = unet\n",
    "#ENC_CHS = (1,64,128,256,512,1024)\n",
    "#DEC_CHS= (1024,512,256,128,64)\n",
    "ENC_CHS = (1,64,128,256,512)\n",
    "DEC_CHS= (512,256,128,64)\n",
    "#model = unet(enc_chs=ENC_CHS, dec_chs=DEC_CHS, num_metr=2)\n",
    "model = unet(enc_chs=ENC_CHS, dec_chs=DEC_CHS, num_metr=NUM_METRICS)\n",
    "#model = unet(retain_dim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k) Early Stopping\n",
    "\n",
    "epoch_score = accuracy at that epoch\n",
    "model_path = where to save the model\n",
    "patience = after how many steps of no progress to stop training\n",
    "\n",
    "Saves one model when beginning, i.e. when no best score is yet available.\n",
    "Why np.copy ? Maybe to turn epoch_score it into np object?\n",
    "\n",
    "IF the accuracy (+delta) decreased: patience counter incremented by one, stopping once patience is reached.\n",
    "Otherwise: continue trianing, reset counter, update best score with new score.\n",
    "\n",
    "Save checkpoint: sved best score is intermittently saved as val_score, new best_score is compared to old val_score and printed out, state_dict is saved at model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.0001): # how big delta?\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "\n",
    "        elif ((EVAL_FN == 'MSE' and score > self.best_score - self.delta) or \n",
    "              (EVAL_FN == 'R2' and score < self.best_score + self.delta)):\n",
    "\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                \"EarlyStopping counter: {} out of {}\".format(\n",
    "                    self.counter, self.patience\n",
    "                )\n",
    "            )\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                #self.counter = 0\n",
    "            \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        model_path = Path(model_path)\n",
    "        parent = model_path.parent\n",
    "        os.makedirs(parent, exist_ok=True)\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print(\n",
    "                \"Validation score improved ({} --> {}). Model saved at at {}!\".format(\n",
    "                    self.val_score, epoch_score, model_path\n",
    "                )\n",
    "            )\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l) Averagemeter\n",
    "\n",
    "Averagemeter plays a role in taking care of the losses.\n",
    "\n",
    "It's initialized in train_one_epoch, having the values set at zero.\n",
    "Reset appears to be unused.\n",
    "It gets updated with the current loss, which is handed as \"val\", and batch_size as n.\n",
    "val is therefore the loss score.\n",
    "sum is previous sum loss score plus val multiplied by batch size.\n",
    "count is previous batches + new one\n",
    "avg is sum of all loss scores divided by count of all batches.\n",
    "\n",
    "Train one epoch is doing what it says it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m) Train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, loss_fn, accumulation_steps=1, device='cuda'):\n",
    "    losses = AverageMeter()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    if accumulation_steps > 1: \n",
    "        optimizer.zero_grad()\n",
    "    tk0 = tqdm(train_loader, total=len(train_loader))\n",
    "    for b_idx, data in enumerate(tk0):\n",
    "        for key, value in data.items():\n",
    "            data[key] = value.to(device)\n",
    "            #print(data['landscape'].size())\n",
    "        if accumulation_steps == 1 and b_idx == 0:\n",
    "            optimizer.zero_grad()\n",
    "        out  = model(data['landscape'])\n",
    "        #print(out.size())\n",
    "        loss = loss_fn(out, data['metric'])\n",
    "        if LOSS_FN == 'RMSE':\n",
    "            loss = torch.sqrt(loss)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            loss.backward()\n",
    "            if (b_idx + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        losses.update(loss.item(), train_loader.batch_size)\n",
    "        tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_one_epoch(train_loader=train_dataloader, model=model, optimizer=optimizer, loss_fn=criterion, accumulation_steps=1, device='cuda'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n) Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from ChatGPT adjusted:\n",
    "# \"When using Mean Squared Error (MSE) loss for image regression in PyTorch,\n",
    "# you can compute the validation accuracy function using the following code:\"\n",
    "# useing torcheval.metrics.MeanSquaredError()\n",
    "# torcheval.metrics.R2Score\n",
    "\n",
    "#https://pytorch.org/torcheval/main/torcheval.metrics.html#regression-metrics\n",
    "from torcheval.metrics import MeanSquaredError, R2Score\n",
    "\n",
    "if EVAL_FN == 'MSE':\n",
    "    eval_metric = MeanSquaredError()\n",
    "    \n",
    "elif EVAL_FN == 'R2':\n",
    "    eval_metric = R2Score()\n",
    "\n",
    "def evaluate(val_loader, model, device='cuda', criterion=eval_metric):\n",
    "    \"\"\"\n",
    "    Computes the validation accuracy of the model on the validation set.\n",
    "    \"\"\"\n",
    "    #losses = AverageMeter()\n",
    "    #val_loss = 0.0\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    tk0 = tqdm(val_loader, total=len(val_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b_idx, data in enumerate(tk0):\n",
    "            for key, value in data.items():\n",
    "                data[key] = value.to(device)\n",
    "                #print(data['landscape'].size())\n",
    "            out   = model(data['landscape'])  \n",
    "                     \n",
    "            # chatgpt suggest flaten:\n",
    "            targets = data['metric']\n",
    "            #inputs_flat = inputs.view(inputs.shape[0], -1)\n",
    "            targets_flat = targets.view(targets.shape[0], -1).cpu()\n",
    "            outputs_flat = out.view(out.shape[0], -1).cpu()\n",
    "            # calculate the loss\n",
    "            #loss = criterion(out, data['metric']).cpu()\n",
    "            #criterion.update(out, data['metric']).cpu()\n",
    "            criterion.update(outputs_flat, targets_flat)#.cpu()            \n",
    "            val_loss = criterion.compute()\n",
    "\n",
    "            #val_loss += loss.item() * landscapes.size(0) # accumulate the validation loss\n",
    "            #losses.update(loss.mean().item(), val_loader.batch_size)\n",
    "            #losses.update(loss.item(), val_loader.batch_size)\n",
    "           \n",
    "        # calculate the average validation loss\n",
    "            #val_loss = val_loss / len(val_loader.dataset)\n",
    "            \n",
    "            tk0.set_postfix(val_score=val_loss)\n",
    "            \n",
    "        criterion.reset()\n",
    "    \n",
    "    # calculate the validation accuracy (in this case, MSE)\n",
    "    #val_accuracy = val_loss\n",
    "    #return losses.avg\n",
    "    return val_loss\n",
    "    #return val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing evaluation function on pretrained model\n",
    "\n",
    "#best_model_path = PRETRAINED_PATH\n",
    "#model.load_state_dict(torch.load(best_model_path))\n",
    "#model = model.to('cuda')\n",
    "#print(evaluate(val_loader=val_dataloader, model=model, device='cuda', criterion=eval_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if PRETRAINED: \n",
    "    model.load_state_dict(torch.load(PRETRAINED_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the last convolutional layer to have 3 output channels\n",
    "#model.head = nn.Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
    "model.head = nn.Conv2d(64, NUM_METRICS_NEW, kernel_size=(1, 1), stride=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=[3,5,6,7,8,9,10,11,13,15], gamma=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o) Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es = EarlyStopping(patience=3, mode='max')\n",
    "es = EarlyStopping(patience=10, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.custom_mae_loss(y_pred, y_true)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8750 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([8, 1, 128, 128])) that is different to the input size (torch.Size([8, 3, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      " 67%|██████▋   | 5824/8750 [20:01<10:05,  4.84it/s, learning_rate=2e-5, loss=0.0175]"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        eval_score = evaluate(val_dataloader, model, criterion=eval_metric)      \n",
    "        scheduler.step()\n",
    "        print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL SCORE: {eval_score}\")\n",
    "        es(eval_score, model, model_path=f\"../notebooks/outputs/saved_models/bst_model{IMG_SIZE}_fold{FOLD_ID}_{np.round(eval_score,4)}.bin\")\n",
    "        #best_model = f\"../data/bst_model{IMG_SIZE}__fold{FOLD_ID}_{np.round(es.best_score,4)}.bin\"\n",
    "        #es(dice, model, model_path=f\"../notebooks/data/bst_model{IMG_SIZE}_fold{FOLD_ID}_{str(np.round(dice,4))[2:]}.bin\")\n",
    "        #best_model = f\"../notebooks/data/saved_models/bst_model{IMG_SIZE}__fold{FOLD_ID}_{np.round(es.best_score,4)}.bin\"\n",
    "        if es.early_stop:\n",
    "            print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n",
    "            break\n",
    "            \n",
    "'''         ## whats this for?  \n",
    "if EVALUATE: \n",
    "    #valid_score = evaluate(val_dataloader, model, metric=metric)\n",
    "    valid_score = evaluate(val_dataloader, model, metric=acc_metric)\n",
    "    print(f\"Valid dice score: {valid_score}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### p) Predict Test Landscapes (single-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class TestDataset():\n",
    "    def __init__(self, sample_sub, image_base_dir, augmentation=None):\n",
    "        self.image_base_dir = image_base_dir\n",
    "        self.image_ids      = sample_sub.ImageId.values\n",
    "        self.augmentation   = augmentation\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_id  = self.image_ids[i]\n",
    "        img_path  = os.path.join(self.image_base_dir, image_id+'.png') \n",
    "        image     = cv2.imread(img_path)\n",
    "        image     = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)       \n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image)\n",
    "            image  = sample['image']\n",
    "\n",
    "        return {\n",
    "            'image': image, \n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL or GRID_SEARCH or GRID_SEARCH_OPTIM:\n",
    "    #test_dataset   = LandscapeMetricsDataset(TEST_DF, TRAIN_IMG_DIR, TRAIN_LBL_DIR)#, TEST_TFMS) \n",
    "    test_dataloader = DataLoader(test_dataset, TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataloader = DataLoader(test_dataset, 8, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    #best_model_path = 'data/bst_model128_fold4_0185.bin'\n",
    "    #best_model_path = 'data/bst_model128_fold4_0.0111.bin'\n",
    "    #best_model_path = 'data/bst_model128_fold4_0.1103.bin'\n",
    "    #best_model_path = 'data/bst_model128_fold4_0.1073.bin'\n",
    "    #best_model_path = 'data/saved_models/bst_model128_fold4_0.0.bin'\n",
    "    #best_model_path = 'data/saved_models/bst_model128_fold8_0.968500018119812.bin'\n",
    "    best_model_path = PRETRAINED_PATH\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    #model.load_state_dict(torch.load('<path_to_bst_model>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if TEST_MODEL:\n",
    "    def post_process(probability, threshold, min_size):\n",
    "        mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n",
    "        num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "        #predictions = np.zeros((1024, 1024), np.float32)\n",
    "        predictions = np.zeros((128, 128), np.float32)    \n",
    "        num = 0\n",
    "        for c in range(1, num_component):\n",
    "            p = (component == c)\n",
    "            if p.sum() > min_size:\n",
    "                predictions[p] = 1\n",
    "                num += 1\n",
    "        return predictions, num\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    def predict(test_dataloader, model, device='cuda'):\n",
    "        predicted_pixels = []\n",
    "        losses = AverageMeter()\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        tk0 = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "        for b_idx, data in enumerate(tk0):\n",
    "            data['landscape'] = data['landscape'].to(device)\n",
    "            out   = model(data['landscape'])\n",
    "            #out   = out.detach().cpu().numpy()[:, 0, :, :] \n",
    "            out   = out.detach().cpu().numpy()#[:, 0, :, :] \n",
    "\n",
    "            for out_ in out:\n",
    "                 predicted_pixels.append(out_)\n",
    "        return predicted_pixels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    predicted_metric = predict(test_dataloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(predicted_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    visualize(test_dataset[4]['landscape'].squeeze(), test_dataset[4]['metric'][0].squeeze())\n",
    "    visualize(test_dataset[4]['metric'][1].squeeze(), test_dataset[4]['metric'][1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    type(predicted_metric)\n",
    "    len(predicted_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    #visualize(predicted_metric[2],predicted_metric[2])\n",
    "    visualize(test_dataset[5]['metric'][0].squeeze(), predicted_metric[5][0].squeeze(), left_title= \"Metric 01\", right_title = \"Prediction 01\")\n",
    "    visualize(test_dataset[5]['metric'][1].squeeze(), predicted_metric[5][1].squeeze(), left_title= \"Metric 02\", right_title = \"Prediction 02\")\n",
    "    #visualize(test_dataset[5]['metric'][2].squeeze(), predicted_metric[5][2].squeeze(), left_title= \"Metric 03\", right_title = \"Prediction 03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    print(set(test_dataset[4]['metric'].squeeze()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    print(set(predicted_metric[4].squeeze()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    difference_loss = test_dataset[5]['metric']-predicted_metric[5]\n",
    "    #difference_loss = abs(test_dataset[5]['metric']-predicted_metric[5])\n",
    "    #(test_dataset[5]['metric']-predicted_metric[5]).squeeze()\n",
    "    \n",
    "    visualize(test_dataset[5]['metric'].squeeze(),difference_loss.squeeze(), left_title= \"Metric\", right_title = \"Loss\")\n",
    "    #visualize(test_dataset[4]['metric'].squeeze(),predicted_metric[4].squeeze(), left_title= \"Metric\", right_title = \"Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL:\n",
    "    print(np.count_nonzero(difference_loss))\n",
    "    #128*128 == 16384\n",
    "    print(set(difference_loss.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### q) Submit to Kaggle - `five fold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def inference_image(model, images, device='cuda'):\n",
    "    images = images.to(device)\n",
    "    predicted = model(images)\n",
    "    masks = torch.sigmoid(predicted)\n",
    "    masks = masks.squeeze(1).cpu().detach().numpy()\n",
    "    return masks\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def inference_model(model, loader, device, use_flip=False):\n",
    "    mask_dict = {}\n",
    "    for data in tqdm(loader):\n",
    "        image_ids = data['image_id']\n",
    "        images    = data['image']\n",
    "        masks = inference_image(model, images, device)\n",
    "        for name, mask in zip(image_ids, masks):\n",
    "            mask_dict[name] = mask.astype(np.float32)\n",
    "    return mask_dict\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Path to pretrained models from https://www.kaggle.com/aroraaman/siimacr-pretrained/\n",
    "checkpoints_list=['../data/bst_model512_fold1_0.9492.bin', \n",
    "                  '../data/bst_model512_fold2_0.9565.bin', \n",
    "                  '../data/bst_model512_fold3_0.9608.bin', \n",
    "                  '../data/bst_model512_fold4_0.9604.bin']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "mask_dict = defaultdict(int)\n",
    "for pred_idx, checkpoint_path in enumerate(checkpoints_list):\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    model.eval()\n",
    "    current_mask_dict = inference_model(model, test_dataloader, device='cuda')\n",
    "    for name, mask in current_mask_dict.items():\n",
    "        mask_dict[name] = (mask_dict[name] * pred_idx + mask) / (pred_idx + 1)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(mask_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def creat_search_run():\n",
    "    \"\"\"\n",
    "    Function to save the Grid Search results.\n",
    "    \"\"\"\n",
    "    num_search_dirs = len(glob.glob('../outputs/search_*'))\n",
    "    search_dirs = f\"../outputs/search_{num_search_dirs+1}\"\n",
    "    os.makedirs(search_dirs)\n",
    "    return search_dirs\n",
    "def save_best_hyperparam(text, path):\n",
    "    \"\"\"\n",
    "    Function to save best hyperparameters in a `.yml` file.\n",
    "    :param text: The hyperparameters dictionary.\n",
    "    :param path: Path to save the hyperparmeters.\n",
    "    \"\"\"\n",
    "    with open(path, 'a') as f:\n",
    "        f.write(f\"{str(text)}\\n\")Model Paths for ensemble\n",
    "- `../data/bst_model512_0.8193.bin` (fold-0)\n",
    "- `../data/bst_model512_fold1_0.9492.bin` (fold-1)\n",
    "- `../data/bst_model512_fold2_0.9565.bin`(fold-2)\n",
    "- `../data/bst_model512_fold3_0.9608.bin` (fold-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r) Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GRID_SEARCH or GRID_SEARCH_OPTIM:\n",
    "    '''\n",
    "    import glob as glob\n",
    "    def creat_search_run():\n",
    "        \"\"\"\n",
    "        Function to save the Grid Search results.\n",
    "        \"\"\"\n",
    "        num_search_dirs = len(glob.glob(' ../notebooks/data/grid_search/search_*'))       \n",
    "        search_dirs = f\" ../notebooks/data/grid_search/search_{num_search_dirs+1}\"\n",
    "        os.makedirs(search_dirs)\n",
    "        return search_dirs\n",
    "    '''\n",
    "    def save_best_hyperparam(text, path):\n",
    "        \"\"\"\n",
    "        Function to save best hyperparameters in a `.yml` file.\n",
    "        :param text: The hyperparameters dictionary.\n",
    "        :param path: Path to save the hyperparmeters.\n",
    "        \"\"\"\n",
    "        #if os.stat(path).st_size == 0:\n",
    "         #   with open(path, 'a') as f:          \n",
    "          #      f.write(f\"{str(text.keys())[11:-2]}\\n\")\n",
    "\n",
    "        with open(path, 'a') as f:  \n",
    "            if os.stat(path).st_size == 0:\n",
    "                f.write(f\"{str(text.keys())[11:-2]}\\n\")\n",
    "\n",
    "            #f.write(f\"{str(text)}\\n\")\n",
    "            f.write(f\"{str(text.values())[13:-2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GRID_SEARCH:\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "    # define the grid search parameters\n",
    "    param_grid = {\n",
    "        #val batch size doesn't really matter?       \n",
    "        'TRAIN_BATCH_SIZE': [8, 16],# 16],# 24]#, 32]#10, 20, 40, 60, 80, 100],  \n",
    "        'LEARNING_RATE': [2e-4],# 2e-3, 0.01],#, 0.01]#,       \n",
    "        'EPOCHS': [10],#, 50, 100],\n",
    "        'LOSS_FN'   : ['MSE'] #['MAE', 'MAE']#, 'RMSE', 'MAE']\n",
    "        #'EVAL_FN' : ['R2']#['MSE', \n",
    "        #'NUM_WORKERS'      = [4, 8, 12]\n",
    "    }\n",
    "    \n",
    "#list(ParameterGrid(param_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2e-5 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GRID_SEARCH:  \n",
    "    \n",
    "    es = EarlyStopping(patience=3, mode='max')\n",
    "    \n",
    "    for param_dict in ParameterGrid(param_grid):\n",
    "        TRAIN_BATCH_SIZE = param_dict['TRAIN_BATCH_SIZE']\n",
    "        LEARNING_RATE = param_dict['LEARNING_RATE']\n",
    "        EPOCHS = param_dict['EPOCHS']\n",
    "        LOSS_FN = param_dict['LOSS_FN']\n",
    "        #EVAL_FN = param_dict['EVAL_FN']\n",
    "\n",
    "            # dataloaders # grid search with smaller learning size, i.e. 10k as in test data?\n",
    "        train_dataloader = DataLoader(test_dataset, TRAIN_BATCH_SIZE, \n",
    "                                      shuffle=True if not USE_SAMPLER else False, \n",
    "                                      num_workers=NUM_WORKERS, \n",
    "                                      sampler=SAMPLER if USE_SAMPLER else None)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[3,5,6,7,8,9,10,11,13,15], gamma=0.75)\n",
    "\n",
    "        if LOSS_FN == 'MAE':\n",
    "            criterion = nn.L1Loss()\n",
    "\n",
    "        # MSE & RMSE\n",
    "        elif LOSS_FN == 'MSE' or LOSS_FN == 'RMSE':\n",
    "            criterion = nn.MSELoss()\n",
    "        \n",
    "        es.counter = 0\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n",
    "            eval_score = evaluate(val_dataloader, model, criterion=eval_metric)      \n",
    "            scheduler.step()\n",
    "            param_dict = {'EPOCH': epoch, 'TRAIN LOSS': loss, 'VAL SCORE': float(eval_score), 'LOSS FN': LOSS_FN, 'BATCH SZ': TRAIN_BATCH_SIZE }\n",
    "            param_string = str(param_dict)\n",
    "            print(param_string)\n",
    "            es(eval_score, model, model_path=f\"../notebooks/data/grid_search/bst_model{IMG_SIZE}_lr:_{LEARNING_RATE}_btch:_{TRAIN_BATCH_SIZE},lfn:_{LOSS_FN}_fold{FOLD_ID}_{np.round(eval_score,4)}.bin\")\n",
    "            #save_best_hyperparam(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL SCORE: {eval_score}, LOSS FN: {LOSS_FN}, BATCH SZ: {TRAIN_BATCH_SIZE}, LR: {LEARNING_RATE} \", f\"../notebooks/data/grid_search/best_param.yml\")\n",
    "            #save_best_hyperparam(param_dict, f\"../notebooks/data/grid_search/best_param.yml\")\n",
    "            save_best_hyperparam(param_dict, f\"../notebooks/data/grid_search/best_param.csv\")\n",
    "\n",
    "            if es.early_stop:\n",
    "                print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n",
    "                #es.counter = 0\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GRID_SEARCH_OPTIM:  \n",
    "    \n",
    "    from torch import optim\n",
    "    es = EarlyStopping(patience=3, mode='max')\n",
    "\n",
    "    # define the grid search parameters\n",
    "    optim_grid = [optim.SGD, optim.RMSprop, optim.Adagrad, optim.Adadelta,\n",
    "                      optim.Adam, optim.Adamax, optim.NAdam]\n",
    "    \n",
    "    \n",
    "    for o in optim_grid:\n",
    "        print(o)\n",
    "        optimizer = o(model.parameters(), lr= LEARNING_RATE)\n",
    "\n",
    "        # dataloaders # grid search with smaller learning size, i.e. 10k as in test data?\n",
    "        train_dataloader = DataLoader(test_dataset, TRAIN_BATCH_SIZE, \n",
    "                                      shuffle=True if not USE_SAMPLER else False, \n",
    "                                      num_workers=NUM_WORKERS, \n",
    "                                      sampler=SAMPLER if USE_SAMPLER else None)\n",
    "\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[3,5,6,7,8,9,10,11,13,15], gamma=0.75)\n",
    "        \n",
    "        es = EarlyStopping(patience=3, mode='max')\n",
    "\n",
    "        #es.counter = 0\n",
    "          \n",
    "        for epoch in range(EPOCHS):\n",
    "            loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n",
    "            eval_score = evaluate(val_dataloader, model, criterion=eval_metric)      \n",
    "            scheduler.step()\n",
    "            param_dict = {'OPTIMIZER': o, 'EPOCH': epoch, 'TRAIN LOSS': loss, 'VAL SCORE': float(eval_score), 'LOSS FN': LOSS_FN, 'BATCH SZ': TRAIN_BATCH_SIZE }\n",
    "            param_string = str(param_dict)\n",
    "            print(param_string)\n",
    "            es(eval_score, model, model_path=f\"../notebooks/data/grid_search_optim/bst_model{IMG_SIZE}_lr:_{LEARNING_RATE}_btch:_{TRAIN_BATCH_SIZE},lfn:_{LOSS_FN}_fold{FOLD_ID}_{np.round(eval_score,4)}.bin\")\n",
    "            save_best_hyperparam(param_dict, f\"../notebooks/data/grid_search_optim/best_optim.csv\")\n",
    "\n",
    "            if es.early_stop:\n",
    "                print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
